Reading the Paper of Generative Pre-Trained Transformer v3(GPT3)

Zero-shot learning v.s One-shot learning v.s Few-shot v.s Fine Tuning learning
https://blog.floydhub.com/n-shot-learning/

> With the term “few-shot learning”, the “few” usually lies between zero and five, meaning that training a model with zero examples of each class is known as zero-shot learning,  one example of each class is one-shot learning, and so on.

> In the N-shot learning field, we have n labeled examples of each K classes, i.e. N * K total examples which we call support set S . We also have to classify Query Set Q , where each example lies in one of the K classes.  N-shot learning has three major sub-fields: zero-shot learning, one-shot learning, and few-shot learning, which each deserve individual attention.


RNN


> LSTM (https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

Autoregressive Model
>  An Autoregressive Model is merely a feed-forward model, which predicts the future word from a set of words given a context.



The architecture of GPT2 / GPT3

1. Word2Vec (https://jalammar.github.io/illustrated-word2vec/)
2. 


Autoencoder

Transfer learning

